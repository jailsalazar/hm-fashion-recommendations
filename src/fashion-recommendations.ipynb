{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H&M Personalized Fashion Recommendations\n",
    "\n",
    "This notebook contains the approach taken for the 2022 H&M Personalized Fashion Recommendations Kaggle competition. \n",
    "\n",
    "*Visit repo README.md for instructions on how to execute notebook locally.*\n",
    "\n",
    "Developed By **Jaileen Salazar**\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Recommendation System\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE PATHS\n",
    "TRANSACTIONS_PATH = '../data/transactions_train.csv'\n",
    "CUSTOMER_PATH = '../data/customers.csv'\n",
    "ARTICLES_PATH = '../data/articles.csv'\n",
    "TEST_PATH = '../data/sample_submission.csv'\n",
    "\n",
    "# FILE FORMATS\n",
    "TRANSACTIONS_HEADERS = ['t_dat', 'customer_id', 'article_id', 'price', 'sales_channel_id']\n",
    "ARTICLE_META_HEADERS = ['article_id', 'product_code', 'prod_name', 'product_type_no', 'product_type_name', 'product_group_name', 'graphical_appearance_no', 'graphical_appearance_name', 'colour_group_code', 'colour_group_name', 'perceived_colour_value_id', 'perceived_colour_value_name', 'perceived_colour_master_id', 'perceived_colour_master_name', 'department_no', 'department_name', 'index_code', 'index_name', 'index_group_no', 'index_group_name', 'section_no', 'section_name', 'garment_group_no', 'garment_group_name', 'detail_desc']\n",
    "SUBMISSION_HEADERS = ['customer_id','prediction']\n",
    "CUSTOMER_META_HEADERS = ['customer_id', 'fashion_news_frequency', 'age', 'postal_code']\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "EMBEDDING_DIM = 32\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(filepath, headers=None):\n",
    "    \"\"\"\n",
    "        Open file, apply preprocessing and return formatted dataframe\n",
    "    \"\"\"\n",
    "    df_data = pd.read_csv(filepath, sep=',', header=0, usecols=headers)\n",
    "    if filepath == CUSTOMER_PATH:\n",
    "        df_data['fashion_news_frequency'] = df_data['fashion_news_frequency'].fillna('NONE')\n",
    "        df_data['age'] = df_data['age'].fillna(99)\n",
    "        df_data['postal_code'] = df_data['postal_code'].fillna(0)\n",
    "\n",
    "    return df_data\n",
    "\n",
    "def save_results(ids, predictions, filename, headers):\n",
    "    \"\"\"\n",
    "        Save predictions to csv file\n",
    "    \"\"\"\n",
    "    data = zip(ids, predictions)\n",
    "    with open(filename, 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion Recommendations Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionRecommendationsModel(tfrs.Model):\n",
    "    def __init__(self, user_model, product_model):\n",
    "        super().__init__()\n",
    "        self.user_model: tf.keras.Model = user_model\n",
    "        self.product_model: tf.keras.Model = product_model\n",
    "        self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        # We pick out the user features and pass them into the user model.\n",
    "        user_embeddings = self.user_model(features[\"customer_id\"])\n",
    "        # And pick out the product features and pass them into the product model,\n",
    "        # getting embeddings back.\n",
    "        positive_product_embeddings = self.product_model(features[\"article_id\"])\n",
    "\n",
    "        # The task computes the loss and the metrics.\n",
    "        return self.task(user_embeddings, positive_product_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = parse_data(filepath=TRANSACTIONS_PATH, headers=TRANSACTIONS_HEADERS)\n",
    "# For training time purposes, only certain time period will be used\n",
    "# Last transaction date is Sep 20 2021\n",
    "transactions = transactions[transactions['t_dat'] >= '2020-09-01']\n",
    "\n",
    "products = parse_data(filepath=ARTICLES_PATH, headers=ARTICLE_META_HEADERS)\n",
    "products['detail_desc'] = products['detail_desc'].fillna('')\n",
    "users = parse_data(filepath=CUSTOMER_PATH)\n",
    "test_ids = parse_data(filepath=TEST_PATH, headers=['customer_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 02:55:23.491115: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Training -> 80,000\n",
    "# Testing -> 20,000\n",
    "transactions = transactions.sort_values(by=['t_dat'])\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(dict(transactions[['customer_id','article_id']].astype(str))).take(80_000)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(dict(transactions[['customer_id','article_id']].astype(str))).skip(80_000).take(20_000)\n",
    "\n",
    "# Remove any duplicates from user_ids and product_ids\n",
    "user_ids = np.unique(users[['customer_id']])\n",
    "product_ids = np.unique(products[['article_id']]).astype(str)\n",
    "\n",
    "# Generate product Dataset for model\n",
    "products_ds = tf.data.Dataset.from_tensor_slices(dict(products[['article_id']].astype(str)))\n",
    "product_map = products_ds.map(lambda x: x['article_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert user_ids to ints for matrix factorization\n",
    "user_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=user_ids, mask_token=None),\n",
    "  # Additional embeddings to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(user_ids) + 1, EMBEDDING_DIM)\n",
    "])\n",
    "\n",
    "# Convert product_ids to ints for matrix factorization\n",
    "product_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=product_ids, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(product_ids) + 1, EMBEDDING_DIM)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics: Compare affinity score that the model calculates for this pair to the scores of all the other possible candidates: if the score for the positive pair is higher than for all other candidates, our model is highly accurate.\n",
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=product_map.batch(BATCH_SIZE).map(product_model)\n",
    ")\n",
    "\n",
    "# Loss\n",
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_model = FashionRecommendationsModel(user_model, product_model)\n",
    "rec_model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train_ds.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test_ds.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/10 [==============================] - 149s 15s/step - factorized_top_k/top_1_categorical_accuracy: 7.2500e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0193 - factorized_top_k/top_10_categorical_accuracy: 0.0299 - factorized_top_k/top_50_categorical_accuracy: 0.0609 - factorized_top_k/top_100_categorical_accuracy: 0.0760 - loss: 70323.5554 - regularization_loss: 0.0000e+00 - total_loss: 70323.5554\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 146s 15s/step - factorized_top_k/top_1_categorical_accuracy: 0.0024 - factorized_top_k/top_5_categorical_accuracy: 0.1141 - factorized_top_k/top_10_categorical_accuracy: 0.1631 - factorized_top_k/top_50_categorical_accuracy: 0.3150 - factorized_top_k/top_100_categorical_accuracy: 0.4044 - loss: 68997.2202 - regularization_loss: 0.0000e+00 - total_loss: 68997.2202\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 150s 15s/step - factorized_top_k/top_1_categorical_accuracy: 0.0062 - factorized_top_k/top_5_categorical_accuracy: 0.2502 - factorized_top_k/top_10_categorical_accuracy: 0.3167 - factorized_top_k/top_50_categorical_accuracy: 0.4786 - factorized_top_k/top_100_categorical_accuracy: 0.5624 - loss: 64022.7610 - regularization_loss: 0.0000e+00 - total_loss: 64022.7610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffbcd6c1ac0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_model.fit(cached_train, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 34s 7s/step - factorized_top_k/top_1_categorical_accuracy: 0.0030 - factorized_top_k/top_5_categorical_accuracy: 0.0099 - factorized_top_k/top_10_categorical_accuracy: 0.0145 - factorized_top_k/top_50_categorical_accuracy: 0.0419 - factorized_top_k/top_100_categorical_accuracy: 0.0645 - loss: 32536.8184 - regularization_loss: 0.0000e+00 - total_loss: 32536.8184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0030499999411404133,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.00989999994635582,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.014499999582767487,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.041850000619888306,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.06445000320672989,\n",
       " 'loss': 29571.80078125,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 29571.80078125}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ScaNN used for its increased performance with large datasets\n",
    "retrieval_index = tfrs.layers.factorized_top_k.ScaNN(rec_model.user_model, k=12)\n",
    "\n",
    "# recommends products out of the entire transactions dataset.\n",
    "retrieval_index = tfrs.layers.factorized_top_k.ScaNN(rec_model.user_model, k=12).index_from_dataset(\n",
    "  tf.data.Dataset.zip((product_map.batch(100), product_map.batch(100).map(rec_model.product_model)))\n",
    ")\n",
    "\n",
    "# Get recommendations.\n",
    "_, raw_preds = retrieval_index(test_ids.customer_id.values)\n",
    "product_predications = raw_preds.numpy().astype(str)\n",
    "product_predications = pd.Series(map(' '.join, product_predications))\n",
    "save_results(test_ids.customer_id.values, product_predications, filename='submission.csv', headers=SUBMISSION_HEADERS)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
