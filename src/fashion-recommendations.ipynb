{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H&M Personalized Fashion Recommendations\n",
    "\n",
    "This notebook contains the approach taken for the 2022 H&M Personalized Fashion Recommendations Kaggle competition. \n",
    "\n",
    "*Visit repo README.md for instructions on how to execute notebook locally.*\n",
    "\n",
    "Developed By **Jaileen Salazar**\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Recommendation System\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE PATHS\n",
    "TRANSACTIONS_PATH = '../data/transactions_train.csv'\n",
    "CUSTOMER_PATH = '../data/customers.csv'\n",
    "ARTICLES_PATH = '../data/articles.csv'\n",
    "TEST_PATH = '../data/sample_submission.csv'\n",
    "\n",
    "# FILE FORMATS\n",
    "TRANSACTIONS_HEADERS = ['t_dat', 'customer_id', 'article_id', 'price', 'sales_channel_id']\n",
    "ARTICLE_META_HEADERS = ['article_id', 'product_code', 'prod_name', 'product_type_no', 'product_type_name', 'product_group_name', 'graphical_appearance_no', 'graphical_appearance_name', 'colour_group_code', 'colour_group_name', 'perceived_colour_value_id', 'perceived_colour_value_name', 'perceived_colour_master_id', 'perceived_colour_master_name', 'department_no', 'department_name', 'index_code', 'index_name', 'index_group_no', 'index_group_name', 'section_no', 'section_name', 'garment_group_no', 'garment_group_name', 'detail_desc']\n",
    "SUBMISSION_HEADERS = ['customer_id','prediction']\n",
    "CUSTOMER_META_HEADERS = ['customer_id', 'fashion_news_frequency', 'age', 'postal_code']\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "EMBEDDING_DIM = 32\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(filepath, headers=None):\n",
    "    \"\"\"\n",
    "        Open file, apply preprocessing and return formatted dataframe\n",
    "    \"\"\"\n",
    "    df_data = pd.read_csv(filepath, sep=',', header=0, usecols=headers)\n",
    "    if filepath == CUSTOMER_PATH:\n",
    "        df_data['fashion_news_frequency'] = df_data['fashion_news_frequency'].fillna('NONE')\n",
    "        df_data['age'] = df_data['age'].fillna(99)\n",
    "        df_data['postal_code'] = df_data['postal_code'].fillna(0)\n",
    "\n",
    "    return df_data\n",
    "\n",
    "def save_results(ids, predictions, filename, headers):\n",
    "    \"\"\"\n",
    "        Save predictions to csv file\n",
    "    \"\"\"\n",
    "    data = zip(ids, predictions)\n",
    "    with open(filename, 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion Recommendations Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionRecommendationsModel(tfrs.Model):\n",
    "    def __init__(self, user_model, product_model):\n",
    "        super().__init__()\n",
    "        self.user_model: tf.keras.Model = user_model\n",
    "        self.product_model: tf.keras.Model = product_model\n",
    "        self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        # We pick out the user features and pass them into the user model.\n",
    "        user_embeddings = self.user_model(features[\"customer_id\"])\n",
    "        # And pick out the product features and pass them into the product model,\n",
    "        # getting embeddings back.\n",
    "        positive_product_embeddings = self.product_model(features[\"article_id\"])\n",
    "\n",
    "        # The task computes the loss and the metrics.\n",
    "        return self.task(user_embeddings, positive_product_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = parse_data(filepath=TRANSACTIONS_PATH, headers=TRANSACTIONS_HEADERS)\n",
    "# For training time purposes, only certain time period will be used\n",
    "# Last transaction date is Sep 21 2020\n",
    "transactions = transactions[transactions['t_dat'] >= '2020-09-01']\n",
    "\n",
    "products = parse_data(filepath=ARTICLES_PATH, headers=ARTICLE_META_HEADERS)\n",
    "products['detail_desc'] = products['detail_desc'].fillna('')\n",
    "users = parse_data(filepath=CUSTOMER_PATH)\n",
    "test_ids = parse_data(filepath=TEST_PATH, headers=['customer_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training -> Before 09/14/2020\n",
    "# Testing -> After 09/14/2020\n",
    "transactions = transactions.sort_values(by=['t_dat'])\n",
    "train_ds = transactions[transactions['t_dat'] <= '2020-09-14']\n",
    "test_ds = transactions[transactions['t_dat'] > '2020-09-14']\n",
    "print('Training Length: ', len(train_ds))\n",
    "print('Testing Length: ', len(test_ds))\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(dict(train_ds[['customer_id','article_id']].astype(str)))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(dict(test_ds[['customer_id','article_id']].astype(str)))\n",
    "\n",
    "# Remove any duplicates from user_ids and product_ids\n",
    "user_ids = np.unique(users[['customer_id']])\n",
    "product_ids = np.unique(products[['article_id']]).astype(str)\n",
    "\n",
    "# Generate product Dataset for model\n",
    "products_ds = tf.data.Dataset.from_tensor_slices(dict(products[['article_id']].astype(str)))\n",
    "product_map = products_ds.map(lambda x: x['article_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert user_ids to ints for matrix factorization\n",
    "user_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=user_ids, mask_token=None),\n",
    "  # Additional embeddings to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(user_ids) + 1, EMBEDDING_DIM)\n",
    "])\n",
    "\n",
    "# Convert product_ids to ints for matrix factorization\n",
    "product_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=product_ids, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(product_ids) + 1, EMBEDDING_DIM)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics: Compare affinity score that the model calculates for this pair to the scores of all the other possible candidates: if the score for the positive pair is higher than for all other candidates, our model is highly accurate.\n",
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=product_map.batch(BATCH_SIZE).map(product_model)\n",
    ")\n",
    "\n",
    "# Loss\n",
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_model = FashionRecommendationsModel(user_model, product_model)\n",
    "rec_model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train_ds.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test_ds.batch(4096).cache()\n",
    "rec_model.fit(cached_train, epochs=EPOCHS)\n",
    "rec_model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScaNN used for its increased performance with large datasets\n",
    "retrieval_index = tfrs.layers.factorized_top_k.ScaNN(rec_model.user_model, k=12)\n",
    "\n",
    "# recommends products out of the entire transactions dataset.\n",
    "retrieval_index = tfrs.layers.factorized_top_k.ScaNN(rec_model.user_model, k=12).index_from_dataset(\n",
    "  tf.data.Dataset.zip((product_map.batch(100), product_map.batch(100).map(rec_model.product_model)))\n",
    ")\n",
    "\n",
    "# Get recommendations\n",
    "_, raw_preds = retrieval_index(test_ids.customer_id.values)\n",
    "product_predications = raw_preds.numpy().astype(str)\n",
    "# Due to type conversions, product ids leading 0 were removed, need to be readded for submission\n",
    "product_predications_formatted = [[y.zfill(10) for y in x] for x in product_predications]\n",
    "product_predications_series = pd.Series(map(' '.join, product_predications_formatted))\n",
    "save_results(test_ids.customer_id.values, product_predications_series, filename='submission.csv', headers=SUBMISSION_HEADERS)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
